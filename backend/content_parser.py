"""
å†…å®¹è§£ææ¨¡å—
æ”¯æŒç½‘é¡µè§£æï¼ˆBeautifulSoupï¼‰å’Œ PDF è§£æï¼ˆPyPDF2ï¼‰
"""

import logging
import requests
from bs4 import BeautifulSoup
from PyPDF2 import PdfReader
from typing import Dict, Any
from config import TIMEOUTS

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ContentParser:
    """å†…å®¹è§£æå™¨"""

    def parse_url(self, url: str) -> Dict[str, Any]:
        """
        è§£æç½‘é¡µå†…å®¹

        Args:
            url: ç½‘é¡µ URL

        Returns:
            åŒ…å«è§£ææ–‡æœ¬å’Œæ—¥å¿—çš„å­—å…¸
        """
        logs = []
        logs.append(f"å¼€å§‹è§£æç½‘å€: {url}")

        try:
            # å‘é€ HTTP è¯·æ±‚ï¼Œä½¿ç”¨æ›´çœŸå®çš„æµè§ˆå™¨è¯·æ±‚å¤´
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1',
                'Cache-Control': 'max-age=0',
                'Referer': 'https://www.google.com/',  # æ·»åŠ  Refererï¼Œä¼ªè£…æˆä»æœç´¢å¼•æ“æ¥çš„
                'DNT': '1'
            }

            # åˆ›å»º session ä»¥ä¿æŒ cookies
            session = requests.Session()
            session.headers.update(headers)

            response = session.get(url, timeout=TIMEOUTS["url_parsing"], allow_redirects=True)
            response.raise_for_status()
            response.encoding = response.apparent_encoding

            logs.append(f"æˆåŠŸè·å–ç½‘é¡µå†…å®¹ï¼ŒçŠ¶æ€ç : {response.status_code}")

            # ä½¿ç”¨ BeautifulSoup è§£æ HTML
            soup = BeautifulSoup(response.text, 'html.parser')

            # ç§»é™¤ script å’Œ style æ ‡ç­¾
            for script in soup(['script', 'style', 'nav', 'footer', 'header']):
                script.decompose()

            # æå–æ–‡æœ¬å†…å®¹
            text = soup.get_text(separator='\n', strip=True)

            # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
            lines = [line.strip() for line in text.split('\n') if line.strip()]
            content = '\n'.join(lines)

            # é™åˆ¶é•¿åº¦ï¼ˆé˜²æ­¢å†…å®¹è¿‡é•¿ï¼‰
            max_length = 10000
            if len(content) > max_length:
                content = content[:max_length] + "\n...(å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­)"
                logs.append(f"å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­è‡³ {max_length} å­—ç¬¦")

            logs.append(f"æˆåŠŸæå–æ–‡æœ¬ï¼Œå…± {len(content)} å­—ç¬¦")

            return {
                "success": True,
                "content": content,
                "logs": logs,
                "source": "url",
                "url": url
            }

        except requests.Timeout:
            error_msg = f"ç½‘é¡µè§£æè¶…æ—¶ï¼ˆ{TIMEOUTS['url_parsing']}ç§’ï¼‰"
            logs.append(f"é”™è¯¯: {error_msg}")
            logger.error(error_msg)
            return {
                "success": False,
                "error": error_msg,
                "logs": logs,
                "source": "url"
            }

        except requests.RequestException as e:
            # æ£€æŸ¥æ˜¯å¦æ˜¯ 403 Forbidden é”™è¯¯
            if "403" in str(e) or "Forbidden" in str(e):
                error_msg = f"è¯¥ç½‘ç«™æ‹’ç»äº†è®¿é—®è¯·æ±‚ï¼ˆ403 Forbiddenï¼‰ã€‚è¿™é€šå¸¸æ˜¯å› ä¸ºç½‘ç«™çš„åçˆ¬è™«ç­–ç•¥é™åˆ¶äº†æœåŠ¡å™¨è®¿é—®ã€‚\n\nğŸ’¡ å»ºè®®ï¼šè¯·å¤åˆ¶ç½‘é¡µæ–‡æœ¬å†…å®¹ï¼Œç›´æ¥ç²˜è´´åˆ°ã€Œè¯é¢˜æ–‡æœ¬ã€è¾“å…¥æ¡†ä¸­ã€‚"
                logs.append(f"è®¿é—®è¢«æ‹’ç»: {url}")
                logger.warning(f"403 Forbidden: {url}")
            else:
                error_msg = f"ç½‘é¡µè¯·æ±‚å¤±è´¥: {str(e)}"
                logs.append(f"é”™è¯¯: {error_msg}")
                logger.error(error_msg)

            return {
                "success": False,
                "error": error_msg,
                "logs": logs,
                "source": "url",
                "error_code": "403" if "403" in str(e) else "network_error"
            }

        except Exception as e:
            error_msg = f"ç½‘é¡µè§£æå¤±è´¥: {str(e)}"
            logs.append(f"é”™è¯¯: {error_msg}")
            logger.error(error_msg)
            return {
                "success": False,
                "error": error_msg,
                "logs": logs,
                "source": "url"
            }

    def parse_pdf(self, pdf_path: str) -> Dict[str, Any]:
        """
        è§£æ PDF æ–‡ä»¶

        Args:
            pdf_path: PDF æ–‡ä»¶è·¯å¾„

        Returns:
            åŒ…å«è§£ææ–‡æœ¬å’Œæ—¥å¿—çš„å­—å…¸
        """
        logs = []
        logs.append(f"å¼€å§‹è§£æ PDF: {pdf_path}")

        try:
            # ä½¿ç”¨ PyPDF2 è¯»å– PDF
            reader = PdfReader(pdf_path)
            num_pages = len(reader.pages)

            logs.append(f"PDF å…± {num_pages} é¡µ")

            # æå–æ‰€æœ‰é¡µé¢çš„æ–‡æœ¬
            all_text = []
            for i, page in enumerate(reader.pages):
                try:
                    text = page.extract_text()
                    if text.strip():
                        all_text.append(text)
                        logs.append(f"æˆåŠŸæå–ç¬¬ {i + 1} é¡µå†…å®¹")
                    else:
                        logs.append(f"è­¦å‘Š: ç¬¬ {i + 1} é¡µæ— æ³•æå–æ–‡æœ¬ï¼ˆå¯èƒ½æ˜¯æ‰«æç‰ˆï¼‰")
                except Exception as e:
                    logs.append(f"è­¦å‘Š: ç¬¬ {i + 1} é¡µæå–å¤±è´¥: {str(e)}")

            if not all_text:
                error_msg = "PDF æ— æ³•æå–æ–‡æœ¬ï¼Œå¯èƒ½æ˜¯æ‰«æç‰ˆ PDFï¼Œä¸æ”¯æŒæ­¤æ ¼å¼"
                logs.append(f"é”™è¯¯: {error_msg}")
                return {
                    "success": False,
                    "error": error_msg,
                    "logs": logs,
                    "source": "pdf"
                }

            content = '\n'.join(all_text)

            # é™åˆ¶é•¿åº¦
            max_length = 10000
            if len(content) > max_length:
                content = content[:max_length] + "\n...(å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­)"
                logs.append(f"å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­è‡³ {max_length} å­—ç¬¦")

            logs.append(f"æˆåŠŸæå–æ–‡æœ¬ï¼Œå…± {len(content)} å­—ç¬¦")

            return {
                "success": True,
                "content": content,
                "logs": logs,
                "source": "pdf",
                "num_pages": num_pages
            }

        except Exception as e:
            error_msg = f"PDF è§£æå¤±è´¥: {str(e)}"
            logs.append(f"é”™è¯¯: {error_msg}")
            logger.error(error_msg)
            return {
                "success": False,
                "error": error_msg,
                "logs": logs,
                "source": "pdf"
            }

    def merge_contents(self, text_input: str = "", url_content: str = "", pdf_content: str = "") -> str:
        """
        åˆå¹¶å¤šç§æ¥æºçš„å†…å®¹

        Args:
            text_input: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
            url_content: ç½‘é¡µè§£æçš„å†…å®¹
            pdf_content: PDF è§£æçš„å†…å®¹

        Returns:
            åˆå¹¶åçš„æ–‡æœ¬
        """
        contents = []

        if text_input and text_input.strip():
            contents.append(f"ã€ç”¨æˆ·è¾“å…¥ã€‘\n{text_input.strip()}")

        if url_content and url_content.strip():
            contents.append(f"ã€ç½‘é¡µå†…å®¹ã€‘\n{url_content.strip()}")

        if pdf_content and pdf_content.strip():
            contents.append(f"ã€PDF å†…å®¹ã€‘\n{pdf_content.strip()}")

        if not contents:
            return "æ²¡æœ‰å¯ç”¨çš„å†…å®¹"

        merged = "\n\n==========\n\n".join(contents)
        logger.info(f"æˆåŠŸåˆå¹¶ {len(contents)} ä¸ªæ¥æºçš„å†…å®¹ï¼Œæ€»é•¿åº¦: {len(merged)}")

        return merged


# å•ä¾‹å®ä¾‹
content_parser = ContentParser()
